<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"/>
</p>

<h3 align="center">
<p>Analysis and Applications of Multi-Scale CNN Feature Maps
</h3>

![Image description](2x2pooling.png)
![Image description](resnet-50.png)
![Image description](dilated.png)

ðŸ¤— Transformers (formerly known as `pytorch-transformers` and `pytorch-pretrained-bert`) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.

### Features

This is a clone of HuggingFace transformers where a new pooling mechanism class called `AveragePooler` has been added to `modeling_bert.py` that could substitute `BertPooler`. Different from `BertPooler` that relies only a fully connected layer on top of the final layer hidden state of `[CLS]` for classification tasks, `AveragePooler` leverages all the final layer hidden states using a customized attention layer. In particular, this customized attention layer, uses the linearly transformed of sum of final layer non `[PAD]` token embeddings as the single query vector. This cause that this customized attention layer only outputs a single output vector. This single output vector could be used for different sequence classification tasks.

This single output embedding vector generated by this customized attention layer can potentially provides a more comprehensive and richer representation of the input sequence, which could results in a better performance of sequence classification tasks. 